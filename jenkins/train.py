import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
import os
import json
import psutil
import threading
import time
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import shutil

# üîπ R√©pertoire de base (o√π- est le script)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# üîπ Chemins absolus pour les donn√©es et mod√®les
DATA_PATH = os.path.join(BASE_DIR, "..", "data", "Iris.csv")
MODELS_DIR = os.path.join(BASE_DIR, "..", "models")
LOGS_DIR = os.path.join(BASE_DIR, "..", "artifacts", "tensorboard")
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)

# üîπ NETTOYAGE DES LOGS EXISTANTS
if os.path.exists(LOGS_DIR):
    shutil.rmtree(LOGS_DIR)
os.makedirs(LOGS_DIR, exist_ok=True)

# üìä Classe pour monitorer CPU/RAM
class ResourceMonitor:
    def __init__(self, model_name):
        self.model_name = model_name
        self.cpu_usage = []
        self.memory_usage = []
        self.memory_mb = []
        self.monitoring = False
        self.thread = None
        self.start_time = None
        self.process = psutil.Process()
        
    def _monitor(self):
        while self.monitoring:
            try:
                cpu = psutil.cpu_percent(interval=0.1)
                mem = psutil.virtual_memory().percent
                mem_mb = self.process.memory_info().rss / (1024 * 1024 * 1024)  # GB
                
                self.cpu_usage.append(cpu)
                self.memory_usage.append(mem)
                self.memory_mb.append(mem_mb)
                
                time.sleep(0.5)
            except:
                pass
    
    def start(self):
        self.monitoring = True
        self.start_time = time.time()
        self.thread = threading.Thread(target=self._monitor, daemon=True)
        self.thread.start()
        print(f"üîç {self.model_name.upper()}: Monitoring d√©marr√©...")
    
    def stop(self):
        self.monitoring = False
        if self.thread:
            self.thread.join(timeout=2)
        duration = time.time() - self.start_time
        
        # Affichage console
        print(f"\nüìä R√âSUM√â RESSOURCES - {self.model_name.upper()}")
        print(f"‚è±Ô∏è  Dur√©e monitoring: {duration:.0f} secondes")
        print(f"üíª CPU moyen: {sum(self.cpu_usage)/len(self.cpu_usage):.1f}% (max: {max(self.cpu_usage):.1f}%)")
        print(f"üß† M√©moire moyenne: {sum(self.memory_usage)/len(self.memory_usage):.1f}% (max: {max(self.memory_usage):.1f}%)")
        print(f"üíæ M√©moire utilis√©e: {sum(self.memory_mb)/len(self.memory_mb):.1f} GB")
        
        return {
            'duration': duration,
            'cpu_mean': sum(self.cpu_usage)/len(self.cpu_usage) if self.cpu_usage else 0,
            'cpu_max': max(self.cpu_usage) if self.cpu_usage else 0,
            'cpu_values': self.cpu_usage,
            'mem_mean': sum(self.memory_usage)/len(self.memory_usage) if self.memory_usage else 0,
            'mem_max': max(self.memory_usage) if self.memory_usage else 0,
            'mem_values': self.memory_usage,
            'mem_gb': sum(self.memory_mb)/len(self.memory_mb) if self.memory_mb else 0
        }

# Fonction pour entra√Æner progressivement et logger les m√©triques
def train_with_logging(model, X_train, y_train, X_test, y_test, model_name, writer, n_steps=20):
    """
    Entra√Æne le mod√®le progressivement en augmentant la taille du dataset
    et log les m√©triques √† chaque √©tape pour cr√©er des courbes
    """
    print(f"üîÑ Entra√Ænement progressif de {model_name}...")
    
    # Cr√©er des subsets progressifs du dataset d'entra√Ænement
    train_sizes = np.linspace(0.1, 1.0, n_steps)
    
    for step, size in enumerate(train_sizes):
        # S√©lectionner un subset du training set
        n_samples = max(10, int(len(X_train) * size))
        X_subset = X_train[:n_samples]
        y_subset = y_train[:n_samples]
        
        # Entra√Æner le mod√®le sur ce subset
        model.fit(X_subset, y_subset)
        
        # Pr√©dire sur le test set
        y_pred = model.predict(X_test)
        
        # Calculer les m√©triques
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
        
        # Logger dans TensorBoard
        writer.add_scalar(f'metrics/accuracy/{model_name}', accuracy, step)
        writer.add_scalar(f'metrics/precision/{model_name}', precision, step)
        writer.add_scalar(f'metrics/recall/{model_name}', recall, step)
        writer.add_scalar(f'metrics/f1_score/{model_name}', f1, step)
        
        # Logger aussi pour la comparaison
        writer.add_scalars('comparison/accuracy', {model_name: accuracy}, step)
        writer.add_scalars('comparison/precision', {model_name: precision}, step)
        writer.add_scalars('comparison/recall', {model_name: recall}, step)
        writer.add_scalars('comparison/f1_score', {model_name: f1}, step)
        
        # Afficher la progression
        if (step + 1) % 5 == 0 or step == n_steps - 1:
            print(f"  Step {step+1}/{n_steps} ({int(size*100)}% donn√©es) - "
                  f"Acc: {accuracy:.3f}, Prec: {precision:.3f}, "
                  f"Rec: {recall:.3f}, F1: {f1:.3f}")
    
    # Retourner les m√©triques finales
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }

# 1. Chargement des donn√©es
iris = pd.read_csv(DATA_PATH)

print("üìä Dataset original:")
print(f"Shape: {iris.shape}")
print(f"Colonnes: {list(iris.columns)}")

# 2. S√©lection des colonnes
feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
target_column = 'Species'

missing_cols = [col for col in feature_columns + [target_column] if col not in iris.columns]
if missing_cols:
    print(f"‚ùå Colonnes manquantes: {missing_cols}")
    print("Colonnes disponibles:", list(iris.columns))
    exit(1)

# 3. Features et target
X = iris[feature_columns].copy()
y = iris[target_column].copy()

print(f"\n‚úÖ Features s√©lectionn√©es: {X.shape}")
print(f"Features: {list(X.columns)}")
print(f"Target: {y.name}")
print(f"Classes uniques: {y.unique()}")

# 4. Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Convertir en numpy arrays pour faciliter le slicing
X_train = X_train.values
y_train = y_train.values

# 5. Sauvegarde du X_test et y_test
TEST_DATA_PATH = os.path.join(BASE_DIR, "..", "data", "iris_test.json")

test_data = {
    "X": X_test.values.tolist(),
    "y": y_test.tolist()
}

with open(TEST_DATA_PATH, "w", encoding="utf-8") as f:
    json.dump(test_data, f, indent=4, ensure_ascii=False)

print(f"üíæ Jeu de test sauvegard√© dans : {TEST_DATA_PATH}")

# 6. D√©finition des mod√®les
models = {
    "svm": SVC(probability=True, random_state=42),
    "random_forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "logistic_regression": LogisticRegression(max_iter=200, random_state=42, multi_class="multinomial")
}

results = {}
resources_stats = {}
metrics_stats = {}

# TensorBoard Writer
writer = SummaryWriter(log_dir=LOGS_DIR)

print("\n" + "="*70)
print("üíª ENTRA√éNEMENT PROGRESSIF AVEC COURBES")
print("="*70)

# 7. Entra√Ænement et √©valuation avec monitoring ET courbes
for idx, (name, model) in enumerate(models.items()):
    print(f"\nüöÄ Entra√Ænement du mod√®le: {name}")
    
    # D√©marrer le monitoring
    monitor = ResourceMonitor(name)
    monitor.start()
    
    # Entra√Ænement progressif avec logging des m√©triques
    final_metrics = train_with_logging(model, X_train, y_train, X_test, y_test, name, writer, n_steps=20)
    
    # Arr√™ter le monitoring
    stats = monitor.stop()
    resources_stats[name] = stats
    
    # Sauvegarder les m√©triques finales
    results[name] = final_metrics['accuracy']
    metrics_stats[name] = final_metrics
    
    print(f"‚úÖ {name} - M√©triques finales:")
    print(f"   Accuracy:  {final_metrics['accuracy']:.3f}")
    print(f"   Precision: {final_metrics['precision']:.3f}")
    print(f"   Recall:    {final_metrics['recall']:.3f}")
    print(f"   F1-Score:  {final_metrics['f1_score']:.3f}")
    
    # üìä R√©sum√© texte pour chaque mod√®le
    text_summary = f"""
**R√âSUM√â COMPLET - {name.upper()}**

**M√âTRIQUES DE PERFORMANCE FINALES:**
- **Accuracy:** {final_metrics['accuracy']:.3f}
- **Precision:** {final_metrics['precision']:.3f}
- **Recall:** {final_metrics['recall']:.3f}
- **F1-Score:** {final_metrics['f1_score']:.3f}

**RESSOURCES CONSOMM√âES:**
- **Dur√©e monitoring:** {stats['duration']:.0f} secondes
- **CPU moyen:** {stats['cpu_mean']:.1f}% (max: {stats['cpu_max']:.1f}%)
- **M√©moire moyenne:** {stats['mem_mean']:.1f}% (max: {stats['mem_max']:.1f}%)
- **M√©moire utilis√©e:** {stats['mem_gb']:.1f} GB
"""
    writer.add_text(f'model_resumes/{name}', text_summary, 0)
    
    # M√©triques scalaires ressources
    writer.add_scalar(f'resources/cpu_mean/{name}', stats['cpu_mean'], 0)
    writer.add_scalar(f'resources/cpu_max/{name}', stats['cpu_max'], 0)
    writer.add_scalar(f'resources/memory_mean/{name}', stats['mem_mean'], 0)
    writer.add_scalar(f'resources/memory_max/{name}', stats['mem_max'], 0)
    writer.add_scalar(f'resources/memory_gb/{name}', stats['mem_gb'], 0)
    writer.add_scalar(f'resources/duration/{name}', stats['duration'], 0)
    
    # Test rapide avec le mod√®le final
    test_data_sample = X_test.iloc[0:1]
    pred = model.predict(test_data_sample)[0]
    proba = model.predict_proba(test_data_sample).max()

    print(f"üß™ Test {name}:")
    print(f"Input: {test_data_sample.values}")
    print(f"Pr√©diction: {pred}, Confiance: {proba:.3f}")

    # Sauvegarde du mod√®le final
    joblib.dump(model, os.path.join(MODELS_DIR, f"{name}_iris_model.pkl"))
    print(f"üíæ Mod√®le sauvegard√©: {os.path.join(MODELS_DIR, f'{name}_iris_model.pkl')}")

# üìä R√âSUM√â GLOBAL dans TensorBoard
global_summary = "# üíª R√âSUM√â COMPLET - TOUS LES MOD√àLES\n\n"

for name in models.keys():
    stats = resources_stats[name]
    metrics = metrics_stats[name]
    global_summary += f"""
## {name.upper()}

**PERFORMANCES:**
- **Accuracy:** {metrics['accuracy']:.3f}
- **Precision:** {metrics['precision']:.3f}
- **Recall:** {metrics['recall']:.3f}
- **F1-Score:** {metrics['f1_score']:.3f}

**RESSOURCES:**
- **Dur√©e:** {stats['duration']:.0f}s
- **CPU moyen:** {stats['cpu_mean']:.1f}% (max: {stats['cpu_max']:.1f}%)
- **M√©moire moyenne:** {stats['mem_mean']:.1f}% (max: {stats['mem_max']:.1f}%)
- **M√©moire utilis√©e:** {stats['mem_gb']:.1f} GB

---
"""

writer.add_text('0_RESUME_GLOBAL', global_summary, 0)

# Comparaisons scalaires ressources
for name in models.keys():
    writer.add_scalars('comparison/cpu_mean', {name: resources_stats[name]['cpu_mean']}, 0)
    writer.add_scalars('comparison/memory_mean', {name: resources_stats[name]['mem_mean']}, 0)

writer.close()

# 8. Sauvegarder infos features
feature_info = {
    'feature_names': feature_columns,
    'n_features': len(feature_columns),
    'models': list(models.keys())
}
joblib.dump(feature_info, os.path.join(MODELS_DIR, "feature_info.pkl"))
print("\nüíæ Info features sauvegard√©es: ", os.path.join(MODELS_DIR, "feature_info.pkl"))

print("\n" + "="*70)
print("üìä R√âSUM√â FINAL DES PERFORMANCES")
print("="*70)
for name in models.keys():
    stats = resources_stats[name]
    metrics = metrics_stats[name]
    print(f"\nüîπ {name.upper()}")
    print(f"  üìà M√âTRIQUES:")
    print(f"     Accuracy:  {metrics['accuracy']:.3f}")
    print(f"     Precision: {metrics['precision']:.3f}")
    print(f"     Recall:    {metrics['recall']:.3f}")
    print(f"     F1-Score:  {metrics['f1_score']:.3f}")
    print(f"  üíª RESSOURCES:")
    print(f"     CPU moyen: {stats['cpu_mean']:.1f}% (max: {stats['cpu_max']:.1f}%)")
    print(f"     RAM moyenne: {stats['mem_mean']:.1f}% (max: {stats['mem_max']:.1f}%)")
    print(f"     M√©moire: {stats['mem_gb']:.1f} GB")
    print(f"     Dur√©e: {stats['duration']:.0f}s")

print("\n" + "="*70)
print(f"üìä TensorBoard logs sauvegard√©s dans: {LOGS_DIR}")
print("üöÄ Pour visualiser: tensorboard --logdir=" + LOGS_DIR)
print("\nüìã GUIDE TENSORBOARD - O√π trouver les COURBES:")
print("  1. Onglet TEXT:")
print("     - '0_RESUME_GLOBAL' : R√©sum√© complet de tous les mod√®les")
print("     - 'model_resumes/' : R√©sum√© individuel par mod√®le")
print("  2. Onglet SCALARS (COURBES D'ENTRA√éNEMENT):")
print("     - 'metrics/accuracy/' : Courbes d'accuracy par mod√®le (20 points)")
print("     - 'metrics/precision/' : Courbes de precision par mod√®le (20 points)")
print("     - 'metrics/recall/' : Courbes de recall par mod√®le (20 points)")
print("     - 'metrics/f1_score/' : Courbes de F1-score par mod√®le (20 points)")
print("     - 'comparison/' : Comparaisons directes entre mod√®les")
print("     - 'resources/' : Consommation CPU/RAM")
print("  3. Vous verrez maintenant de VRAIES COURBES au lieu de points isol√©s!")
print("="*70)